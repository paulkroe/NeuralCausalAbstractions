{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bicf1Ifz68r9"
      },
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWC3nKWe68sB",
        "outputId": "35983696-a633-45d2-d55c-cb84634f5b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NeuralCausalAbstractions' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/paulkroe/NeuralCausalAbstractions.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd NeuralCausalAbstractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJG447qD7Hlb",
        "outputId": "2317f913-c61c-4f55-fc91-0aa708aea0cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NeuralCausalAbstractions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning==2.2.0\n",
        "!pip install python-mnist\n",
        "!pip install tensorboard==2.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u61DnFDS7Met",
        "outputId": "bbdb9016-fdeb-41c8-a9ac-1d6bddabdfd8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning==2.2.0\n",
            "  Downloading pytorch_lightning-2.2.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (2024.10.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning==2.2.0)\n",
            "  Downloading torchmetrics-1.6.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (4.12.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning==2.2.0)\n",
            "  Downloading lightning_utilities-0.14.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (3.11.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning==2.2.0) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->pytorch_lightning==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->pytorch_lightning==2.2.0) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (3.10)\n",
            "Downloading pytorch_lightning-2.2.0-py3-none-any.whl (800 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.0-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.2-py3-none-any.whl (931 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch_lightning-2.2.0 torchmetrics-1.6.2\n",
            "Collecting python-mnist\n",
            "  Downloading python_mnist-0.7-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: python-mnist\n",
            "Successfully installed python-mnist-0.7\n",
            "Collecting tensorboard==2.14.0\n",
            "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (1.70.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard==2.14.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (75.1.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.0) (3.2.2)\n",
            "Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: google-auth-oauthlib, tensorboard\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0 tensorboard-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auto Encoder Conditional"
      ],
      "metadata": {
        "id": "EfoC8nKNQlIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.main CIFAR sampling cifar gan --h-layers 3 --h-size 2 --scale-h-size --scale-u-size --batch-norm --gan-mode wgan --gan-arch biggan --repr auto_enc_conditional --rep-size 64 --rep-image-only --rep-h-layers 3 --rep-h-size 128 --img-size 32 --rep-no-decoder --rep-max-epochs=100 --rep-patience=100 --max-epochs=100 --patience=100 --gpu=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuZVZN6k7ZgN",
        "outputId": "0809276c-002a-4de2-9bb5-1ab594a67a5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pipeline': 'gan', 'mode': 'sampling', 'transform': 'cifar', 'lr': 0.0001, 'alpha': 0.99, 'data-bs': 128, 'ncm-bs': 128, 'grad-acc': 1, 'max-epochs': 100, 'patience': 100, 'h-layers': 3, 'h-size': 2, 'scale-h-size': True, 'feature-maps': 64, 'u-size': 1, 'scale-u-size': True, 'neural-pu': False, 'batch-norm': True, 'gan-mode': 'wgan', 'gan-arch': 'biggan', 'disc-type': 'standard', 'disc-lr': 0.0002, 'disc-h-layers': 2, 'disc-h-size': -1, 'd-iters': 1, 'grad-clamp': 0.01, 'gp-weight': 10.0, 'gp-one-side': False, 'img-size': 32, 'repr': 'auto_enc_conditional', 'rep-size': 64, 'rep-type': 'real', 'rep-image-only': True, 'rep-lr': 0.0001, 'rep-bs': 128, 'rep-grad-acc': 1, 'rep-h-layers': 3, 'rep-h-size': 128, 'rep-feature-maps': 64, 'rep-class-lambda': 0.1, 'rep-temperature': 1.0, 'rep-contrast-lambda': 0.1, 'rep-no-decoder': True, 'rep-max-epochs': 100, 'rep-patience': 100, 'identify': False, 'normalize': True, 'id-reruns': 1, 'max-lambda': 0.01, 'min-lambda': 0.0001, 'use-tau': False, 'img-query': True, 'verbose': False}\n",
            "[running] out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dat/cifar/cifar-10-python.tar.gz\n",
            "100% 170M/170M [00:14<00:00, 11.6MB/s]\n",
            "Extracting ./dat/cifar/cifar-10-python.tar.gz to ./dat/cifar\n",
            "Files already downloaded and verified\n",
            "Param count for Gs initialized parameters: 1865091\n",
            "/content/NeuralCausalAbstractions/src/pipeline/repr_pipeline.py:110: UserWarning: Enforces unsup cont loss\n",
            "  warnings.warn(\"Enforces unsup cont loss\", UserWarning)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training representation model...\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Missing logger folder: out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0/logs/rep_model/lightning_logs\n",
            "2025-03-11 16:06:24.076999: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-11 16:06:24.094206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741709184.112883    3788 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741709184.118451    3788 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-11 16:06:24.137867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | RepresentationalNN | 3.2 M \n",
            "1 | loss          | MSELoss            | 0     \n",
            "2 | classify_loss | BCELoss            | 0     \n",
            "-----------------------------------------------------\n",
            "3.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.2 M     Total params\n",
            "12.667    Total estimated model params size (MB)\n",
            "Epoch 99: 100% 78/78 [00:43<00:00,  1.79it/s, v_num=0, train_loss=4.770, recon_loss=4.770, label_loss=0.000]`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "Epoch 99: 100% 78/78 [00:43<00:00,  1.79it/s, v_num=0, train_loss=4.770, recon_loss=4.770, label_loss=0.000]\n",
            "/content/NeuralCausalAbstractions/src/run/standard_ncm_runner.py:199: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = T.load(rep_checkpoint.best_model_path)  # Find best model\n",
            "Done training representation model!\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0/logs/ncm/lightning_logs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type               | Params\n",
            "--------------------------------------------------\n",
            "0 | ncm        | GAN_NCM            | 230 K \n",
            "1 | disc       | Discriminator      | 697   \n",
            "2 | repr_model | RepresentationalNN | 3.2 M \n",
            "--------------------------------------------------\n",
            "3.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.4 M     Total params\n",
            "13.594    Total estimated model params size (MB)\n",
            "Epoch 99: 100% 78/78 [00:03<00:00, 23.58it/s, v_num=0, train_loss=1e+8, G_loss=0.00997, D_loss=-5.26e-6]`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "Epoch 99: 100% 78/78 [00:03<00:00, 23.57it/s, v_num=0, train_loss=1e+8, G_loss=0.00997, D_loss=-5.26e-6]\n",
            "MovieWriter imagemagick unavailable; using Pillow instead.\n",
            "[done] out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv out/CIFAR ."
      ],
      "metadata": {
        "id": "QaCtfCFR738p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.experiment.cifar_experiment --dir=/content/NeuralCausalAbstractions/CIFAR/ --num-samples=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QkEByg277S0",
        "outputId": "2ccd815d-18ee-4c65-fb8c-fc28e72122fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "groud truth for P(old = 1 | do(one_hot_animal = 'horse')) = 0.1736\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Monte Carlo Estimation for P(old = 1 | do(one_hot_animal = 'horse')) with 100 samples 0.2200\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Param count for Gs initialized parameters: 1865091\n",
            "/content/NeuralCausalAbstractions/src/pipeline/repr_pipeline.py:110: UserWarning: Enforces unsup cont loss\n",
            "  warnings.warn(\"Enforces unsup cont loss\", UserWarning)\n",
            "/content/NeuralCausalAbstractions/src/metric/model_utility.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  rep_m.load_state_dict(T.load('{}/best_rep.th'.format(d)))\n",
            "/content/NeuralCausalAbstractions/src/metric/model_utility.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  m.load_state_dict(T.load(\"{}/best.th\".format(d)))\n",
            "Estimation for P(old = 1 | do(one_hot_animal = 'horse')) with 100 samples 0.4400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auto Encoder Notrain\n"
      ],
      "metadata": {
        "id": "XvFacGcsQsj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.main CIFAR sampling cifar gan --h-layers 3 --h-size 2 --scale-h-size --scale-u-size --batch-norm --gan-mode wgan --gan-arch biggan --repr auto_enc_notrain --rep-size 64 --rep-image-only --rep-h-layers 3 --rep-h-size 128 --img-size 32 --rep-no-decoder --rep-max-epochs=100 --rep-patience=100 --max-epochs=100 --patience=100 --gpu=0"
      ],
      "metadata": {
        "id": "AqlILEG6QsOH",
        "outputId": "596c3d25-282c-417f-cbe8-8fe3d510c95c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pipeline': 'gan', 'mode': 'sampling', 'transform': 'cifar', 'lr': 0.0001, 'alpha': 0.99, 'data-bs': 128, 'ncm-bs': 128, 'grad-acc': 1, 'max-epochs': 100, 'patience': 100, 'h-layers': 3, 'h-size': 2, 'scale-h-size': True, 'feature-maps': 64, 'u-size': 1, 'scale-u-size': True, 'neural-pu': False, 'batch-norm': True, 'gan-mode': 'wgan', 'gan-arch': 'biggan', 'disc-type': 'standard', 'disc-lr': 0.0002, 'disc-h-layers': 2, 'disc-h-size': -1, 'd-iters': 1, 'grad-clamp': 0.01, 'gp-weight': 10.0, 'gp-one-side': False, 'img-size': 32, 'repr': 'auto_enc_notrain', 'rep-size': 64, 'rep-type': 'real', 'rep-image-only': True, 'rep-lr': 0.0001, 'rep-bs': 128, 'rep-grad-acc': 1, 'rep-h-layers': 3, 'rep-h-size': 128, 'rep-feature-maps': 64, 'rep-class-lambda': 0.1, 'rep-temperature': 1.0, 'rep-contrast-lambda': 0.1, 'rep-no-decoder': True, 'rep-max-epochs': 100, 'rep-patience': 100, 'identify': False, 'normalize': True, 'id-reruns': 1, 'max-lambda': 0.01, 'min-lambda': 0.0001, 'use-tau': False, 'img-query': True, 'verbose': False}\n",
            "[running] out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Param count for Gs initialized parameters: 1865091\n",
            "/content/NeuralCausalAbstractions/src/pipeline/repr_pipeline.py:110: UserWarning: Enforces unsup cont loss\n",
            "  warnings.warn(\"Enforces unsup cont loss\", UserWarning)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training representation model...\n",
            "Done training representation model!\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Missing logger folder: out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0/logs/ncm/lightning_logs\n",
            "2025-03-11 17:32:20.068726: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-11 17:32:20.084790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741714340.103351   37175 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741714340.108786   37175 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-11 17:32:20.127027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type               | Params\n",
            "--------------------------------------------------\n",
            "0 | ncm        | GAN_NCM            | 230 K \n",
            "1 | disc       | Discriminator      | 697   \n",
            "2 | repr_model | RepresentationalNN | 3.2 M \n",
            "--------------------------------------------------\n",
            "3.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.4 M     Total params\n",
            "13.592    Total estimated model params size (MB)\n",
            "Epoch 99: 100% 78/78 [00:03<00:00, 24.16it/s, v_num=0, train_loss=1e+8, G_loss=0.010, D_loss=-5.44e-6]`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "Epoch 99: 100% 78/78 [00:03<00:00, 24.15it/s, v_num=0, train_loss=1e+8, G_loss=0.010, D_loss=-5.44e-6]\n",
            "MovieWriter imagemagick unavailable; using Pillow instead.\n",
            "[done] out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv out/CIFAR ."
      ],
      "metadata": {
        "id": "fuSaPcn5Q4Ib"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.experiment.cifar_experiment --dir=/content/NeuralCausalAbstractions/CIFAR/ --num-samples=100"
      ],
      "metadata": {
        "id": "mZFfgbu6Q6l2",
        "outputId": "613aa223-ea87-41f4-9fab-a83bcb0671a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "groud truth for P(old = 1 | do(one_hot_animal = 'horse')) = 0.1736\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Monte Carlo Estimation for P(old = 1 | do(one_hot_animal = 'horse')) with 100 samples 0.1500\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Param count for Gs initialized parameters: 1865091\n",
            "/content/NeuralCausalAbstractions/src/pipeline/repr_pipeline.py:110: UserWarning: Enforces unsup cont loss\n",
            "  warnings.warn(\"Enforces unsup cont loss\", UserWarning)\n",
            "/content/NeuralCausalAbstractions/src/metric/model_utility.py:114: UserWarning: No representation model found. Using untrained pipeline.\n",
            "  warnings.warn(\"No representation model found. Using untrained pipeline.\")\n",
            "/content/NeuralCausalAbstractions/src/metric/model_utility.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  m.load_state_dict(T.load(\"{}/best.th\".format(d)))\n",
            "Estimation for P(old = 1 | do(one_hot_animal = 'horse')) with 100 samples 0.5200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auto Encoder"
      ],
      "metadata": {
        "id": "AZNDVSnWQ_a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.main CIFAR sampling cifar gan --h-layers 3 --h-size 2 --scale-h-size --scale-u-size --batch-norm --gan-mode wgan --gan-arch biggan --repr auto_enc --rep-size 64 --rep-image-only --rep-h-layers 3 --rep-h-size 128 --img-size 32 --rep-no-decoder --rep-max-epochs=100 --rep-patience=100 --max-epochs=100 --patience=100 --gpu=0"
      ],
      "metadata": {
        "id": "w4t_BN0VQ-kP",
        "outputId": "0655203e-906a-4cea-d80b-f73e8827bc4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pipeline': 'gan', 'mode': 'sampling', 'transform': 'cifar', 'lr': 0.0001, 'alpha': 0.99, 'data-bs': 128, 'ncm-bs': 128, 'grad-acc': 1, 'max-epochs': 100, 'patience': 100, 'h-layers': 3, 'h-size': 2, 'scale-h-size': True, 'feature-maps': 64, 'u-size': 1, 'scale-u-size': True, 'neural-pu': False, 'batch-norm': True, 'gan-mode': 'wgan', 'gan-arch': 'biggan', 'disc-type': 'standard', 'disc-lr': 0.0002, 'disc-h-layers': 2, 'disc-h-size': -1, 'd-iters': 1, 'grad-clamp': 0.01, 'gp-weight': 10.0, 'gp-one-side': False, 'img-size': 32, 'repr': 'auto_enc', 'rep-size': 64, 'rep-type': 'real', 'rep-image-only': True, 'rep-lr': 0.0001, 'rep-bs': 128, 'rep-grad-acc': 1, 'rep-h-layers': 3, 'rep-h-size': 128, 'rep-feature-maps': 64, 'rep-class-lambda': 0.1, 'rep-temperature': 1.0, 'rep-contrast-lambda': 0.1, 'rep-no-decoder': True, 'rep-max-epochs': 100, 'rep-patience': 100, 'identify': False, 'normalize': True, 'id-reruns': 1, 'max-lambda': 0.01, 'min-lambda': 0.0001, 'use-tau': False, 'img-query': True, 'verbose': False}\n",
            "[running] out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Param count for Gs initialized parameters: 1865091\n",
            "/content/NeuralCausalAbstractions/src/pipeline/repr_pipeline.py:110: UserWarning: Enforces unsup cont loss\n",
            "  warnings.warn(\"Enforces unsup cont loss\", UserWarning)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training representation model...\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Missing logger folder: out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0/logs/rep_model/lightning_logs\n",
            "2025-03-11 17:38:46.858837: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-11 17:38:46.875067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741714726.893800   44873 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741714726.899232   44873 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-11 17:38:46.917888: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | RepresentationalNN | 3.2 M \n",
            "1 | loss          | MSELoss            | 0     \n",
            "2 | classify_loss | BCELoss            | 0     \n",
            "-----------------------------------------------------\n",
            "3.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.2 M     Total params\n",
            "12.666    Total estimated model params size (MB)\n",
            "Epoch 28:   0% 0/78 [00:00<?, ?it/s, v_num=0, train_loss=4.800, recon_loss=4.800]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv out/CIFAR ."
      ],
      "metadata": {
        "id": "qKk0LK3MRJQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.experiment.cifar_experiment --dir=/content/NeuralCausalAbstractions/CIFAR/ --num-samples=100"
      ],
      "metadata": {
        "id": "b0K01u68RKsO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}