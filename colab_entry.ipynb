{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulkroe/NeuralCausalAbstractions/blob/main/colab_entry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bicf1Ifz68r9"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWC3nKWe68sB",
        "outputId": "ea1cf66c-3299-42e2-a6f4-17954e373b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NeuralCausalAbstractions'...\n",
            "remote: Enumerating objects: 402, done.\u001b[K\n",
            "remote: Counting objects: 100% (402/402), done.\u001b[K\n",
            "remote: Compressing objects: 100% (261/261), done.\u001b[K\n",
            "remote: Total 402 (delta 255), reused 271 (delta 133), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (402/402), 121.69 KiB | 1.96 MiB/s, done.\n",
            "Resolving deltas: 100% (255/255), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/paulkroe/NeuralCausalAbstractions.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd NeuralCausalAbstractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJG447qD7Hlb",
        "outputId": "5e2a1726-b843-4c0e-f5b4-f2f8b51ec7ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NeuralCausalAbstractions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning==2.2.0\n",
        "!pip install python-mnist\n",
        "!pip install tensorboard==2.14.0\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u61DnFDS7Met",
        "outputId": "1c184885-b629-4d32-c45a-d43f1078d8a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning==2.2.0\n",
            "  Downloading pytorch_lightning-2.2.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (2025.3.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning==2.2.0)\n",
            "  Downloading torchmetrics-1.7.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.2.0) (4.12.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning==2.2.0)\n",
            "  Downloading lightning_utilities-0.14.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (3.11.14)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning==2.2.0) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->pytorch_lightning==2.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch_lightning==2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->pytorch_lightning==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->pytorch_lightning==2.2.0) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.2.0) (3.10)\n",
            "Downloading pytorch_lightning-2.2.0-py3-none-any.whl (800 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.2-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.0-py3-none-any.whl (960 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch_lightning-2.2.0 torchmetrics-1.7.0\n",
            "Collecting python-mnist\n",
            "  Downloading python_mnist-0.7-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: python-mnist\n",
            "Successfully installed python-mnist-0.7\n",
            "Collecting tensorboard==2.14.0\n",
            "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (1.71.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard==2.14.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (75.1.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.0) (3.2.2)\n",
            "Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: google-auth-oauthlib, tensorboard\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0 tensorboard-2.14.0\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.24.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "p-YgoXKb1L1G",
        "outputId": "067524ff-8681-4aad-8388-9bad486ffc99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaulkroe\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Parents"
      ],
      "metadata": {
        "id": "jLPEjNva1hNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.main CIFAR sampling cifar gan \\\n",
        "    --n-samples 150000 \\\n",
        "    --max-epochs 100 \\\n",
        "    --patience 100 \\\n",
        "    --h-layers 3 \\\n",
        "    --h-size 2 \\\n",
        "    --scale-h-size \\\n",
        "    --scale-u-size \\\n",
        "    --batch-norm \\\n",
        "    --img-size 32 \\\n",
        "    --data-bs 2048 \\\n",
        "    --gan-mode wgan \\\n",
        "    --gan-arch biggan \\\n",
        "    --rep-bs 2048 \\\n",
        "    --rep-size 64 \\\n",
        "    --rep-image-only \\\n",
        "    --rep-h-layers 3 \\\n",
        "    --rep-h-size 128 \\\n",
        "    --rep-max-epochs 25 \\\n",
        "    --rep-patience 25 \\\n",
        "    --disc-lr 0.00005 \\\n",
        "    --disc-h-layers 4 \\\n",
        "    --disc-h-size 256 \\\n",
        "    --gpu 0 \\\n",
        "    --eval-samples 250 \\\n",
        "    --wandb \\\n",
        "    --wandb-project-name NeuralCausalAbstractions \\\n",
        "    --wandb-org-name paulkroe \\\n",
        "    --rep-pred-parents"
      ],
      "metadata": {
        "id": "ZlcGxGwm2Zjc",
        "outputId": "e1cd14cd-9857-4ab8-b05e-a05cb77611f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pipeline': 'gan', 'mode': 'sampling', 'transform': 'cifar', 'lr': 0.0001, 'alpha': 0.99, 'data-bs': 1024, 'ncm-bs': 128, 'grad-acc': 1, 'max-epochs': 100, 'patience': 100, 'h-layers': 3, 'h-size': 2, 'scale-h-size': True, 'feature-maps': 64, 'u-size': 1, 'scale-u-size': True, 'neural-pu': False, 'batch-norm': True, 'gan-mode': 'wgan', 'gan-arch': 'biggan', 'disc-type': 'standard', 'disc-lr': 0.0004, 'disc-h-layers': 4, 'disc-h-size': 256, 'd-iters': 1, 'grad-clamp': 0.01, 'gp-weight': 10.0, 'gp-one-side': False, 'img-size': 32, 'no-repr': False, 'rep-size': 64, 'rep-type': 'real', 'rep-image-only': True, 'rep-lr': 0.0001, 'rep-bs': 1024, 'rep-grad-acc': 1, 'rep-h-layers': 3, 'rep-h-size': 128, 'rep-feature-maps': 64, 'rep-class-lambda': 0.1, 'rep-temperature': 1.0, 'rep-contrast-lambda': 0.1, 'rep-max-epochs': 25, 'rep-patience': 25, 'rep-reconstruct': False, 'rep-unsup-contrastive': False, 'rep-sup-contrastive': False, 'rep-pred-parents': True, 'rep-train': True, 'identify': False, 'normalize': True, 'id-reruns': 1, 'max-lambda': 0.01, 'min-lambda': 0.0001, 'use-tau': False, 'eval-samples': 1000, 'img-query': True, 'verbose': False, 'wandb': True, 'wandb-project-name': 'NeuralCausalAbstractions', 'wandb-org-name': 'paulkroe'}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaulkroe\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/NeuralCausalAbstractions/wandb/run-20250327_130600-qv8k785b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwinter-frog-202\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/paulkroe/NeuralCausalAbstractions\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/paulkroe/NeuralCausalAbstractions/runs/qv8k785b\u001b[0m\n",
            "[running] out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=150000-trial_index=0\n",
            "100% 170M/170M [00:02<00:00, 74.8MB/s]\n",
            "Param count for Gs initialized parameters: 1865091\n",
            "Using accelerator: gpu with devices: [0]\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training representation model...\n",
            "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Missing logger folder: out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=150000-trial_index=0/logs/rep_model/lightning_logs\n",
            "2025-03-27 13:08:06.768910: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-27 13:08:06.785415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743080886.804090    3794 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743080886.809658    3794 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-27 13:08:06.829114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | RepresentationalNN | 3.2 M \n",
            "1 | loss          | MSELoss            | 0     \n",
            "2 | classify_loss | BCELoss            | 0     \n",
            "3 | con_loss      | SupConLoss         | 0     \n",
            "-----------------------------------------------------\n",
            "3.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.2 M     Total params\n",
            "12.667    Total estimated model params size (MB)\n",
            "Epoch 0: 100% 146/146 [00:20<00:00,  7.28it/s, v_num=0, train_loss=0.0402, pred_parents_loss=0.0402]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 1: 100% 146/146 [00:20<00:00,  7.06it/s, v_num=0, train_loss=0.0373, pred_parents_loss=0.0373]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 2: 100% 146/146 [00:20<00:00,  7.09it/s, v_num=0, train_loss=0.0353, pred_parents_loss=0.0353]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 3: 100% 146/146 [00:19<00:00,  7.55it/s, v_num=0, train_loss=0.033, pred_parents_loss=0.033]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 4: 100% 146/146 [00:19<00:00,  7.48it/s, v_num=0, train_loss=0.0312, pred_parents_loss=0.0312]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 5: 100% 146/146 [00:19<00:00,  7.62it/s, v_num=0, train_loss=0.0311, pred_parents_loss=0.0311]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 6: 100% 146/146 [00:19<00:00,  7.48it/s, v_num=0, train_loss=0.0296, pred_parents_loss=0.0296]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 7: 100% 146/146 [00:19<00:00,  7.61it/s, v_num=0, train_loss=0.0284, pred_parents_loss=0.0284]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 8: 100% 146/146 [00:19<00:00,  7.41it/s, v_num=0, train_loss=0.0268, pred_parents_loss=0.0268]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 9: 100% 146/146 [00:19<00:00,  7.44it/s, v_num=0, train_loss=0.026, pred_parents_loss=0.026]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 10: 100% 146/146 [00:19<00:00,  7.47it/s, v_num=0, train_loss=0.0247, pred_parents_loss=0.0247]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 11: 100% 146/146 [00:19<00:00,  7.37it/s, v_num=0, train_loss=0.0238, pred_parents_loss=0.0238]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 12: 100% 146/146 [00:19<00:00,  7.41it/s, v_num=0, train_loss=0.0226, pred_parents_loss=0.0226]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 13: 100% 146/146 [00:19<00:00,  7.44it/s, v_num=0, train_loss=0.0218, pred_parents_loss=0.0218]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 14: 100% 146/146 [00:19<00:00,  7.51it/s, v_num=0, train_loss=0.0209, pred_parents_loss=0.0209]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 15: 100% 146/146 [00:19<00:00,  7.47it/s, v_num=0, train_loss=0.0188, pred_parents_loss=0.0188]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 16: 100% 146/146 [00:19<00:00,  7.42it/s, v_num=0, train_loss=0.0195, pred_parents_loss=0.0195]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 17: 100% 146/146 [00:19<00:00,  7.44it/s, v_num=0, train_loss=0.0176, pred_parents_loss=0.0176]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 18: 100% 146/146 [00:19<00:00,  7.49it/s, v_num=0, train_loss=0.016, pred_parents_loss=0.016]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 19: 100% 146/146 [00:19<00:00,  7.61it/s, v_num=0, train_loss=0.0151, pred_parents_loss=0.0151]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 20: 100% 146/146 [00:19<00:00,  7.43it/s, v_num=0, train_loss=0.0129, pred_parents_loss=0.0129]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 21: 100% 146/146 [00:19<00:00,  7.45it/s, v_num=0, train_loss=0.014, pred_parents_loss=0.014]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 22: 100% 146/146 [00:19<00:00,  7.38it/s, v_num=0, train_loss=0.0123, pred_parents_loss=0.0123]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 23: 100% 146/146 [00:19<00:00,  7.48it/s, v_num=0, train_loss=0.0117, pred_parents_loss=0.0117]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "Epoch 24: 100% 146/146 [00:19<00:00,  7.42it/s, v_num=0, train_loss=0.0124, pred_parents_loss=0.0124]/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
            "Epoch 24: 100% 146/146 [01:11<00:00,  2.04it/s, v_num=0, train_loss=0.0124, pred_parents_loss=0.0124]\n",
            "Done training representation model!\n",
            "Using accelerator: gpu with devices: [0]\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=150000-trial_index=0/logs/ncm/lightning_logs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type               | Params\n",
            "--------------------------------------------------\n",
            "0 | ncm        | GAN_NCM            | 230 K \n",
            "1 | disc       | Discriminator      | 218 K \n",
            "2 | repr_model | RepresentationalNN | 3.2 M \n",
            "--------------------------------------------------\n",
            "3.6 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.6 M     Total params\n",
            "14.464    Total estimated model params size (MB)\n",
            "Epoch 57:  55% 80/146 [00:04<00:03, 17.27it/s, v_num=0, train_loss=1e+8, G_loss=0.0319, D_loss=-1.35e-5]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv out/CIFAR CIFAR_pred_parents"
      ],
      "metadata": {
        "id": "QaCtfCFR738p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.experiment.cifar_experiment \\\n",
        "    --dir=/content/NeuralCausalAbstractions/CIFAR_pred_parents/ \\\n",
        "    --num-samples=1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QkEByg277S0",
        "outputId": "43e2e8bc-c0fc-4346-bc5e-1b404ee970d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "groud truth for P(old = 1 | do(one_hot_animal = 'horse')) = 0.1736\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Monte Carlo Estimation for P(old = 1 | do(one_hot_animal = 'horse')) with 1000 samples 0.1750\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Param count for Gs initialized parameters: 1865091\n",
            "/content/NeuralCausalAbstractions/src/metric/model_utility.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  rep_m.load_state_dict(T.load('{}/best_rep.th'.format(d)))\n",
            "/content/NeuralCausalAbstractions/src/metric/model_utility.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  m.load_state_dict(T.load(\"{}/best.th\".format(d)))\n",
            "Estimation for P(old = 1 | do(one_hot_animal = 'horse')) with 1000 samples 0.9620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Contrastive"
      ],
      "metadata": {
        "id": "z8NNFhjo4y85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.main CIFAR sampling cifar gan \\\n",
        "    --max-epochs 100 \\\n",
        "    --patience 100 \\\n",
        "    --h-layers 3 \\\n",
        "    --h-size 2 \\\n",
        "    --scale-h-size \\\n",
        "    --scale-u-size \\\n",
        "    --batch-norm \\\n",
        "    --img-size 32 \\\n",
        "    --data-bs 512 \\\n",
        "    --gan-mode wgan \\\n",
        "    --gan-arch biggan \\\n",
        "    --rep-size 64 \\\n",
        "    --rep-image-only \\\n",
        "    --rep-h-layers 3 \\\n",
        "    --rep-h-size 128 \\\n",
        "    --rep-max-epochs 100 \\\n",
        "    --rep-patience 100 \\\n",
        "    --gpu 0 \\\n",
        "    --wandb \\\n",
        "    --wandb-project-name NeuralCausalAbstractions \\\n",
        "    --wandb-org-name paulkroe \\\n",
        "    --rep-unsup-contrastive"
      ],
      "metadata": {
        "id": "1C3YAMT95Coo",
        "outputId": "73d05794-f584-4293-9491-522a75060be4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pipeline': 'gan', 'mode': 'sampling', 'transform': 'cifar', 'lr': 0.0001, 'alpha': 0.99, 'data-bs': 512, 'ncm-bs': 128, 'grad-acc': 1, 'max-epochs': 100, 'patience': 100, 'h-layers': 3, 'h-size': 2, 'scale-h-size': True, 'feature-maps': 64, 'u-size': 1, 'scale-u-size': True, 'neural-pu': False, 'batch-norm': True, 'gan-mode': 'wgan', 'gan-arch': 'biggan', 'disc-type': 'standard', 'disc-lr': 0.0002, 'disc-h-layers': 2, 'disc-h-size': -1, 'd-iters': 1, 'grad-clamp': 0.01, 'gp-weight': 10.0, 'gp-one-side': False, 'img-size': 32, 'no-repr': False, 'rep-size': 64, 'rep-type': 'real', 'rep-image-only': True, 'rep-lr': 0.0001, 'rep-bs': 512, 'rep-grad-acc': 1, 'rep-h-layers': 3, 'rep-h-size': 128, 'rep-feature-maps': 64, 'rep-class-lambda': 0.1, 'rep-temperature': 1.0, 'rep-contrast-lambda': 0.1, 'rep-max-epochs': 100, 'rep-patience': 100, 'rep-reconstruct': False, 'rep-unsup-contrastive': True, 'rep-sup-contrastive': False, 'rep-pred-parents': False, 'rep-train': True, 'identify': False, 'normalize': True, 'id-reruns': 1, 'max-lambda': 0.01, 'min-lambda': 0.0001, 'use-tau': False, 'img-query': True, 'verbose': False, 'wandb': True, 'wandb-project-name': 'NeuralCausalAbstractions', 'wandb-org-name': 'paulkroe'}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaulkroe\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/NeuralCausalAbstractions/wandb/run-20250313_003511-5fxwwe3h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mradiant-pine-21\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/paulkroe/NeuralCausalAbstractions\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/paulkroe/NeuralCausalAbstractions/runs/5fxwwe3h\u001b[0m\n",
            "[running] out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Param count for Gs initialized parameters: 1865091\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training representation model...\n",
            "Missing logger folder: out/CIFAR/gen=AgeCifarDataGenerator-pipeline=GANPipeline-model=GAN_NCM-n_samples=10000-trial_index=0/logs/rep_model/lightning_logs\n",
            "2025-03-13 00:35:30.258906: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741826130.277534   16426 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741826130.282928   16426 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-13 00:35:30.301577: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | RepresentationalNN | 3.2 M \n",
            "1 | loss          | MSELoss            | 0     \n",
            "2 | classify_loss | BCELoss            | 0     \n",
            "-----------------------------------------------------\n",
            "3.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.2 M     Total params\n",
            "12.666    Total estimated model params size (MB)\n",
            "Epoch 45:  26% 20/78 [00:10<00:31,  1.84it/s, v_num=0, train_loss=4.780, unsup_contrastive_loss=4.780]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv out/CIFAR CIFAR_unsup_contrastive"
      ],
      "metadata": {
        "id": "gf0dCTcw5R3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.experiment.cifar_experiment \\\n",
        "    --dir=/content/NeuralCausalAbstractions/CIFAR_unsup_contrastive/ \\\n",
        "    --num-samples=1000"
      ],
      "metadata": {
        "id": "TJfUwkYP5ZKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Parents AND Unsupervised Contrastive"
      ],
      "metadata": {
        "id": "iZp9q2Ub5ral"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.main CIFAR sampling cifar gan \\\n",
        "    --max-epochs 100 \\\n",
        "    --patience 100 \\\n",
        "    --h-layers 3 \\\n",
        "    --h-size 2 \\\n",
        "    --scale-h-size \\\n",
        "    --scale-u-size \\\n",
        "    --batch-norm \\\n",
        "    --img-size 32 \\\n",
        "    --data-bs 512 \\\n",
        "    --gan-mode wgan \\\n",
        "    --gan-arch biggan \\\n",
        "    --rep-size 64 \\\n",
        "    --rep-image-only \\\n",
        "    --rep-h-layers 3 \\\n",
        "    --rep-h-size 128 \\\n",
        "    --rep-max-epochs 100 \\\n",
        "    --rep-patience 100 \\\n",
        "    --gpu 0 \\\n",
        "    --wandb \\\n",
        "    --wandb-project-name NeuralCausalAbstractions \\\n",
        "    --wandb-org-name paulkroe \\\n",
        "    --rep-pred-parents \\\n",
        "    --rep-unsup-contrastive"
      ],
      "metadata": {
        "id": "9MQJIswq52qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv out/CIFAR CIFAR_pred_parents_unsup_contrastive"
      ],
      "metadata": {
        "id": "rFoqyy087WwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.experiment.cifar_experiment \\\n",
        "    --dir=/content/NeuralCausalAbstractions/CIFAR_pred_parents_unsup_contrastive/ \\\n",
        "    --num-samples=1000"
      ],
      "metadata": {
        "id": "R8G6KWwz7b3K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}